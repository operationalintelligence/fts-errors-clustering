{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec hyperparameters search \n",
    "\n",
    "**Objective**\n",
    "\n",
    "The idea is to train multiple Word2Vec models based on a grid search over the hyperparameters of the model. \n",
    "The results are then saved in the specified Hadoop path for subsequent processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T14:48:43.905131Z",
     "start_time": "2020-03-19T14:48:37.691497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.1 ms, sys: 22.1 ms, total: 63.2 ms\n",
      "Wall time: 6.19 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://28692e1c5667:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sample_app_optimize_word2vec</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f04dca749e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"sample_app_optimize_word2vec\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer errors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T14:55:46.395523Z",
     "start_time": "2020-03-19T14:53:06.081179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 188 ms, sys: 51.8 ms, total: 239 ms\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "# 18th October 2019\n",
    "day = \"2020/03/09\"\n",
    "\n",
    "# FTS data path\n",
    "path_list = ['/project/monitoring/archive/fts/raw/complete/2020/03/{:0>2}/*'.format(i) for i in range(9,13)]\n",
    "\n",
    "# load the data in the json file\n",
    "all_transfers = spark.read.json(path_list)\n",
    "\n",
    "# retrieve just data\n",
    "all_transfers = all_transfers.select(\"data.*\")\n",
    "\n",
    "# filter errors only\n",
    "errors = all_transfers.filter(all_transfers[\"t_final_transfer_state_flag\"] == 0)\n",
    "\n",
    "# add row id and select only relevant variables\n",
    "errors = errors.withColumn(\"msg_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATLAS data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T14:58:28.828382Z",
     "start_time": "2020-03-19T14:55:46.403089Z"
    }
   },
   "outputs": [],
   "source": [
    "errors_atlas = errors.filter(errors[\"vo\"]==\"atlas\").select(\n",
    "    \"msg_id\", \"t__error_message\", \"src_hostname\", \"dst_hostname\", \"timestamp_tr_comp\", \"timestamp_tr_st\")\n",
    "\n",
    "# sample 100 random rows\n",
    "n = errors_atlas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:08:53.840291Z",
     "start_time": "2020-03-19T15:08:53.832359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of errors: 1795737\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of errors: {}\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: read pre-saved data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T23:30:37.843061Z",
     "start_time": "2020-03-12T23:30:25.430392Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# errors = spark.read.json(\"sample_app_test_train.json\").select(\n",
    "#     \"msg_id\", \"t__error_message\", \"src_hostname\", \"dst_hostname\", \"timestamp_tr_comp\")\n",
    "\n",
    "# # visualize data\n",
    "# # errors.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:15:43.726911Z",
     "start_time": "2020-03-19T15:15:43.405465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 107 ms, sys: 42.7 ms, total: 150 ms\n",
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# overwrite for memory reasons and delete errors_atlas\n",
    "errors = errors_atlas\n",
    "del errors_atlas\n",
    "\n",
    "from language_models import tokenizer\n",
    "err_tks = tokenizer(errors, err_col=\"t__error_message\", id_col=\"msg_id\")\n",
    "\n",
    "# visualize tokenization\n",
    "# err_tks.toPandas().head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set metadata and hyperparameters grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import time\n",
    "\n",
    "# to track data interval of training\n",
    "today = datetime.date.today() # day of the training\n",
    "data_window = \"9-13mar2020\" # time period of data analysied\n",
    "w2v_log = \"results/sample_app/w2v_models\" # Hadoop base path\n",
    "\n",
    "vs_list = [100,150,200,250]\n",
    "mc_list = [100, 500]\n",
    "ws_list = [5, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search over specified hyperparameters' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T21:29:11.128668Z",
     "start_time": "2020-03-19T16:49:18.207915Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for vecotor_size=100, window_size=5 and min_count=100\n",
      "Started at: 2020-03-19 17:49:18\n",
      "\n",
      "\n",
      "Time elapsed: 15 minutes and 2 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=100, window_size=8 and min_count=100\n",
      "Started at: 2020-03-19 18:04:20\n",
      "\n",
      "\n",
      "Time elapsed: 15 minutes and 29 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=100, window_size=5 and min_count=500\n",
      "Started at: 2020-03-19 18:19:50\n",
      "\n",
      "\n",
      "Time elapsed: 13 minutes and 41 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=100, window_size=8 and min_count=500\n",
      "Started at: 2020-03-19 18:33:31\n",
      "\n",
      "\n",
      "Time elapsed: 16 minutes and 2 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=150, window_size=5 and min_count=100\n",
      "Started at: 2020-03-19 18:49:34\n",
      "\n",
      "\n",
      "Time elapsed: 15 minutes and 55 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=150, window_size=8 and min_count=100\n",
      "Started at: 2020-03-19 19:05:29\n",
      "\n",
      "\n",
      "Time elapsed: 16 minutes and 45 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=150, window_size=5 and min_count=500\n",
      "Started at: 2020-03-19 19:22:14\n",
      "\n",
      "\n",
      "Time elapsed: 14 minutes and 46 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=150, window_size=8 and min_count=500\n",
      "Started at: 2020-03-19 19:37:01\n",
      "\n",
      "\n",
      "Time elapsed: 16 minutes and 39 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=200, window_size=5 and min_count=100\n",
      "Started at: 2020-03-19 19:53:41\n",
      "\n",
      "\n",
      "Time elapsed: 15 minutes and 12 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=200, window_size=8 and min_count=100\n",
      "Started at: 2020-03-19 20:08:53\n",
      "\n",
      "\n",
      "Time elapsed: 21 minutes and 5 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=200, window_size=5 and min_count=500\n",
      "Started at: 2020-03-19 20:29:58\n",
      "\n",
      "\n",
      "Time elapsed: 19 minutes and 11 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=200, window_size=8 and min_count=500\n",
      "Started at: 2020-03-19 20:49:10\n",
      "\n",
      "\n",
      "Time elapsed: 20 minutes and 26 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=250, window_size=5 and min_count=100\n",
      "Started at: 2020-03-19 21:09:36\n",
      "\n",
      "\n",
      "Time elapsed: 22 minutes and 14 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=250, window_size=8 and min_count=100\n",
      "Started at: 2020-03-19 21:31:51\n",
      "\n",
      "\n",
      "Time elapsed: 22 minutes and 36 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=250, window_size=5 and min_count=500\n",
      "Started at: 2020-03-19 21:54:27\n",
      "\n",
      "\n",
      "Time elapsed: 16 minutes and 10 seconds.\n",
      "------------------------------------------------------------\n",
      "Training for vecotor_size=250, window_size=8 and min_count=500\n",
      "Started at: 2020-03-19 22:10:38\n",
      "\n",
      "\n",
      "Time elapsed: 18 minutes and 32 seconds.\n",
      "------------------------------------------------------------\n",
      "CPU times: user 13 s, sys: 3.48 s, total: 16.5 s\n",
      "Wall time: 4h 39min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import language_models\n",
    "import importlib\n",
    "importlib.reload(language_models)\n",
    "\n",
    "for vs in vs_list:\n",
    "    for mc in mc_list:\n",
    "        for ws in ws_list:\n",
    "            start_time = time.time()\n",
    "            start_time_string = datetime.datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            print(\"Training for vecotor_size={}, window_size={} and min_count={}\".format(vs, ws, mc))\n",
    "            print(\"Started at: {}\\n\".format(start_time_string))\n",
    "\n",
    "            # train word2vec\n",
    "            w2v_model = language_models.train_w2v(err_tks, tks_col=\"stop_token_1\", id_col=\"msg_id\", out_col='message_vector',\n",
    "                                                  vec_size=vs, min_count=mc, mode=\"overwrite\", win_size=ws, n_cores=12,\n",
    "                                                  save_path=\"{}/data_window_{}/train_date_{}\".format(w2v_log, data_window, today))\n",
    "\n",
    "            print(\"\\nTime elapsed: {} minutes and {} seconds.\".format(int((time.time() - start_time)/60), \n",
    "                                                                      int((time.time() - start_time)%60)))\n",
    "            print('--'*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
